# Algorithms and Data Structures

## Important notes and links

---
---

## 3.1 Что такое HashMap и зачем он нужен

* **Python**: `dict`
* **Java**: `HashMap`
* **C++**: `std::unordered_map`
* **JavaScript**: `Object` or `Map`
* **PHP**: `array` (associative arrays)

Коллизий в хеш функциях невозможно избежать.

## 3.3 Базовая реализация HashMap

Примерный принцип работы динамического массива:

* Если массив заполнен - увеличить его размер вдвое
* Если массив заполнен менее чем на четверть - уменьшить его размер вдвое

HashMap также использует эти принципы. При изменении размера массива, также
пересчитываются индексы добавленных значений.

## 3.4 Разрешение коллизий: Открытая Адресация

Один из способов разрешения коллизии поставить новую запись на следующий индекс
внутреннего массива или далее если и он занят.

## 4.1 Оценка асимптотической сложности алгоритмов

Если алгоритм работает всегда за одно и тоже время, говорят, что время работы
алгоритма константа.

**Скорость работы алгоритма по O(n):**

* Константное время - O(1)
* Линейное время - O(n)
* Квадратичное время - O(n^2)

## 4.2 Динамические массивы

При фиксированном увеличении внутреннего массива который находится в  динамическом
массиве, сложность добавления будет O(n^2). При удваивании размера, сложность будет
O(n).

## 4.3 Односвязные списки

Список позволяет добавлять элементы в конец и начало, со сложностью O(1).

Для нахождения N-ого элемента с конца, можно использовать метод "Два указателя".

Ссылку в списке можно иметь не только на первый, но и на последний и на средний
и на какой то другой элемент списка.

## 5.3 Очередь

Очередь можно реализовать с помощью циклического массива и указателей.
При создании очереди на основе массива, можно быстро получать значение любого
элемента очереди. Также внутренние механизмы позволяют использовать кэш для
массива, что также ведет к ускоренной работе с ним.

## 5.4 Альтернативные методы реализации очереди

Очередь можно реализовать с помощью:

* Списка
* Зацикленного массива
* 2 стеков

Очередь из стеков можно сделать перенося элементы из одного стека в другой, если
второй стек пустой.

## 5.5 Дек

На английском пишется как `Deque` или `Dequeue` (Устаревшее).

Задачи требующие использование дека, встречаются не очень часто.

Дек можно реализовать с помощью:

* Списка
* Зацикленного массива
* Двух стеков

## 6.1 Сравнение, компараторы

Компаратор - функция для сравнения данных.

## 6.3 Сортировка выбором

Алгоритм сортировки выборкой заключается в том, что находится самый маленький
элемент и ставится на определенное место и так для каждой позиции массива.

Время исполнения всегда `n**2 / 2`, даже в самом лучшем случае.

Алгоритм может хорошо себя показать если можно дешево сравнивать элементы, но
дорого их менять местами. Также алгоритм можно задействовать, если важно не
время работы, а количество перемещений (меньше чем за n обменов, элементы
не отсортировать)

## 6.4 Сортировка вставками

В этом алгоритме мы берем пустой массив и вставляем в него каждый элемент из
массива, который нужно отсортировать, так чтобы после вставки массив был отсортирован.

Сортировка вставками является стабильной, т.е. с помощью неё можно отсортировать
уже отсортированный по другому значению массив без потери предыдущей сортировки.

## 6.5 Сортировка чисел подсчетом

Доказано, что реализовать сортировку, которая использует только сравнение,
быстрее чем O(n*log(n)) нельзя.

Сортировка подсчетом считается одним из самых быстрых алгоритмов.

Суть алгоритма сортировки подсчетом в подсчете частоты встречаемого элемента.
Алгоритм хорош для небольшого количества возможных значений.

Сложность алгоритма = O(n).
Алгоритм можно использовать, если значения меньше приемлемого ограничения.

## 6.6 Сортировка объектов подсчетом

Чтобы использовать более сложные данные при сортировке подсчетом, можно вместо подсчета
частоты сохранять данные в массив.

## 7.5 Merge Sort. Слияние

Merge Sort имеет сложность по времени O(n * log(n)) и сложность по памяти O(n).
Сортировка заключается в рекурсивном слиянии массивов до массива из одного элемента.

### Сравнение Merge Sort с Quick Sort

1. **Quick Sort:**
   * **Pros:** Average-case O(n * log n), in-place space O(log n), generally faster
     for most datasets.
   * **Cons:** Worst-case O(n^2) (mitigated by good pivot strategies), not stable.

2. **Merge Sort:**
   * **Pros:** Consistent O(n * log n) time complexity, stable, good for linked
     lists and external sorting.
   * **Cons:** Requires additional space O(n), more complex to implement.

**Summary:** Use quicksort for in-place, fast average-case sorting with manageable
worst-case risks. Use merge sort for consistent performance, stability, and when
working with data structures like linked lists or external data.

## 8.1 Быстрая сортировка

Самый большой недостаток Merge Sort это сложность по памяти в O(n).

Quick Sort обычно работает быстрее Merge Sort.

### Медиана

Если все элементы массива различны, то медиана - это такое число, что половина
из элементов массива больше него, а другая половина меньше.

В более общем случае медиану можно найти, упорядочив элементы массива по возрастанию
или убыванию и взяв средний элемент.

### Quick Sort

Обычно в алгоритмах с левой и правой границей, левая граница включительно, а
правая нет.

## 8.2 Разделяющий элемент. Часть 1

Хотя в худшем случае Quick Sort может работать за `n^2`, его асимптотика
будет `O(n * log(n))`.

Среднюю величину можно выбирать разными способами, но рекомендуется выбирать
случайно, чтобы не дать возможность злоумышленникам влиять на скорость процесса.

Если массив сортируется сильно медленнее обычного, можно использовать метод с
помощью которого можно запустить Merge Sort.

## 8.4 K-ая порядковая статистика

Для нахождения K-ого элемента, можно использовать алгоритм Quick Sort. Асимптотика
это алгоритма будет O(n).

## 8.5 Время работы алгоритмов

Порой асимптотика не отображает реальной картины скорости работы алгоритма.

При выполнении программы, компилятор и процессор оптимизируют код
(кэш, буферизация, предсказатель переходов и т.д.), что нужно тоже иметь в виду.

## 9.1 Введение в жадные алгоритмы

**Жадный алгоритм** - это метод решения оптимизационных задач, который строит решение
шаг за шагом, принимая на каждом шаге локально оптимальное решение, то есть такое
решение, которое кажется наилучшим в данный момент. Основная идея жадного
алгоритма заключается в том, что локально оптимальные решения приведут к
глобально оптимальному решению.

## 9.2 Задача про полоски

Бывает для решения не получается придумать контр-пример или доказательство.
Доказать что задача не имеет жадного решения нельзя.
Можно провести стресс-тестирование.

## 9.3 Стресс тестирование

Суть стресс теста в том чтобы написать точно работающий но медленный алгоритм и
сравнить результаты с быстрым алгоритмом, в котором мы не уверены.

Стресс тест можно также написать для поиска багов.

## 9.6 Алгоритм Хаффмана  

Это алгоритм оптимального префиксного
кодирования алфавита.

### Префиксный код в теории кодирования

Это код со словом переменной длины, имеющий такое свойство: если в код входит слово
a, то для любой непустой строки b слова ab в коде не существует. Хотя префиксный
код состоит из слов разной длины, эти слова можно записывать без разделительного
символа.

## 10.1 Двоичные деревья поиска

### Возможные операции

* Вставка по ключу
* Удаление по ключу
* Обращение к элементу
* Обход элементов по возрастанию ключей
* Поиск следующего за ключом
* Поиск минимума/максимума
* Поиск первого элемента, большего заданного
* ...

### Приятные бонусы

* Время работы не зависит от рандома
* Памяти нужно ровно столько, сколько элементов храним
* Асимптотика времени работы не амортизированная

В стандартных
библиотеках
● Java: `TreeMap`
● C++: `std::map`
● Python: `collections.OrderedDict`

## 10.2 Добавление в дерево поиска

Двоичное дерево поиска гарантирует глубину не более логарифма.

При добавлении элемента без балансировки дерева ухудшает эффективность работы с деревом.

## 10.4 Удаление элементов из дерева поиска

Принцип удаления элемента из дерева:

* Если у элемента нет потомков - просто удаляем его
* Если у элемента один потомок - на место старого элемента ставим потомка
* Если у элемента несколько потомков - идем один раз вправо, затем до конца влево
  и найденный элемент меняем со старым (то есть находим минимальный элемент,
  который больше чем старый элемент)

## 10.5 Получение следующего элемента

### Следующий элемент (in-order successor)

1. **Правое поддерево существует**: Идем в правое поддерево и находим самый левый
   узел.
2. **Правое поддерево отсутствует**: Идем вверх по дереву, пока не встретим узел,
   который является левым ребенком своего родителя.

### Предыдущий элемент (in-order predecessor)

1. **Левое поддерево существует**: Идем в левое поддерево и находим самый правый
   узел.
2. **Левое поддерево отсутствует**: Идем вверх по дереву, пока не встретим узел,
   который является правым ребенком своего родителя.

## 10.6 Обход дерева поиска

Вывод всего дерева поиска можно довольно просто реализовать с помощью рекурсии,
начиная с левого потомка корня.

Проверить, что дерево является двоичным деревом поиска, **не получиться** если просто
проверять, что левый потомок меньше родителя, а правый потомок больше.

**Контр-пример:**

```text
    10
   /
  3
   \
    15
```

Для корректной проверки, нужна правая и левая граница. Если поддерево левое, все
элементы должны быть меньше или равны значению родителя поддерева, и симметрично
для правого поддерева элементы должны быть больше или равны родителю.

Эту задачу часто спрашивают на собеседованиях.

## 11.1 Граф

**Граф** - это математическая структура, состоящая из множества вершин (узлов) и
множества рёбер (связей) между этими вершинами. Графы используются для
моделирования и анализа различных систем и процессов, где важны отношения
между объектами.

**Неориентированным графом** G называется пара G=(V,E),
где V - множество вершин, а E⊂{{v,u}:v,u∈V} - множество рёбер.

**Дерево** - связный граф без циклов.

**Связный граф** - граф в котором все вершины связаны
(т.е. От одной вершины можно добраться до другой)

**Корневое дерево** - это дерево у которого одна вершина выделена как **корень**.

**Глубина вершины** - это характеристика того насколько далека вершина от корня.

У корня глубина вершины равна 0.

## 11.2 Хранение дерева

Способы хранения дерева:

* С помощью ссылок (например индексов массива)
* Записывая рёбра
* Записывая предка

Хранение с помощью массива, а не ссылок, обычно быстрее, благодаря кэшированию.

## 11.3 Обход в глубину (DFS - Depth First Search)

Самый большой минус рекурсивного обхода в глубину это большой расход стека,
который ограничен.
По этой причине не рекомендуется использовать обход в глубину при очень большой
глубине дерева.

**Поддерево вершины** - это сама вершина и её потомки.

## 11.4 Время входа и выхода

С помощью **данных времени входа и выхода**, можно определить, что одна вершина лежит
в поддереве другой за O(1), просто по тому, что интервал одной вершины находится
внутри интервала другой.

## 11.5 Наименьший общий предок

Найти наименьшего общего предка можно за O(n), где n это глубина дерева.

Это время можно уменьшить до O(1), с помощью Эйлерова обхода и разреженных таблиц
(чтобы это ни значило).

Также это время можно уменьшить до O(log n), если сделать предвычисления перехода
к родителю с шагами в степени 2 (2, 4, 8, 16 и т.д.)

## 11.6 Поиск в ширину

BFS - Breadth-First Search

Поиск в ширину обходит все вершины на одной глубине начиная с глубины 0 и до последней.

Реализовать поиск в ширину можно с помощью очереди или двух массивов.

Поиск в ширину работает за O(n + m) и требует памяти O(n) в худшем случае.

Алгоритм обхода в ширину нерекурсивный, а значит, не требует
дополнительной памяти на стеке для обработки рекурсии.

## Куча (Heap)

Куча - это структура данных, которая обязательно поддерживает 3 операции:

1. Добавить элемент
2. Получить значение минимального элемента
3. Удалить минимальный элемент

В куче также и наоборот можно получать значение максимального элемента.

### Асимптотика

1. Добавить элемент O(log N)
2. Получить значение минимального элемента O(1)
3. Удалить минимальный элемент O(log N)

## 12.2 Идея

**Куча** - это двоичное дерево, в котором в корне лежит минимальный элемент, а потомки
любой вершины больше чем эта вершина.

**Сбалансированная куча** - куча, где все слои, кроме последнего, полностью заполнены,
а в последнем слое вершины заполнены слева направо.
Она даёт возможность массив для хранения кучи без ссылок.

```php
<?php

$leftChildIndex = $currentNodeIndex * 2 + 1;
$rightChildIndex = $currentNodeIndex * 2 + 2;
$parentIndex = ($currentNodeIndex - 1) / 2
```

## 12.3 Добавление элемента

Добавление элемента выполняется за O(log n).

Для добавления нужно новый элемент поставить на следующее по порядку место в конец.
Затем нужно сбалансировать кучу, протолкнув элемент вверх, пока родитель не будет
меньше нового элемента.

## 12.4 Удаление элемента

Удаление элемента выполняется за O(log n).

Для удаления минимального элементы нужно перенести последнюю вершину в корень и
протолкнуть её вниз.

Для удаления элементы из середины, нужно просеивать его как вверх, так и вниз.

## 12.5 Сортировка кучей

**Сортировка кучей** - выполняется за O(n log n).

Если использовать массив на вход, сортировка не будет требовать дополнительной памяти.

На YouTube есть хорошее видео от `udiprod` на тему сортировки кучи.

## 12.7 Очередь с приоритетами

Для изменения элемента в куче можно хранить ссылки на элемент в дополнительном
массиве или хеш таблице.

Это можно использовать для изменения элементов в электронной очереди.

## 12.8 Рандомизированная куча

Кучи можно объединять. С помощью объединения можно удалять, добавлять и делать
другие вещи. Куча должна работать на ссылках и не будет сбалансированной.

Добавление это объединение кучи и кучи из одного элемента.
Удаление это объединение двух поддеревьев потомков.

Объединение происходит за O(log n).

## 13.1 Бор (Trie)

Бор (Trie) - это дерево для хранения строк, где каждый путь от корня к листу
представляет строку. Узлы соответствуют символам, а ребра связывают узлы. Бор
позволяет эффективно искать слова, проверять префиксы и экономить память с помощью
сжатия.

Поиск наличия строки в боре выполняется за O(m), где m - длина строки.

В боре также можно хранить вместе целые строки и префиксы, для этого вершины помечаются
разными флагами (например: точка для целой строки, звездочка для префикса).

## 13.3 Бор. Применения

С помощью бора можно отсортировать массив строк за O(n * l), где n длина массива,
а l длина строки.

В каждой вершине можно хранить какую нибудь информацию.

## 13.4 Суффиксное дерево

Суффиксное дерево, это бор в который добавили все суффиксы строки
(например для `abcd`: `abcd`, `bcd`, `cd`, `d`).

С его помощью можно быстро проверять есть ли подстрока в строке.

Преподсчёт работает за O(n^2). Алгоритм Укконена позволяет построить сжатое
суффиксное дерево за O(n) требуя O(n) памяти.

## 13.5 B-дерево

B-дерево - это сбалансированное поисковое дерево, в котором каждый узел может
содержать несколько ключей и иметь несколько дочерних узлов, что обеспечивает
эффективное выполнение операций поиска, вставки и удаления за логарифмическое время.

B-дерево оптимизировано для работы с жестким диском, так как выполняет к нему меньше
запросов.

B-дереву можно задать свойство t, при этом количество ключей будет
`t - 1 < k < 2t - 1`, а количество потомков `t < a < 2t`.

Свойство t нужно выбирать так, чтобы за один запрос к жесткому диску
загрузилась вся вершина.

Количество потомков в B-дереве равно количеству ключей, плюс один.

### Свойства

1. Ключи в каждом узле упорядочены по неубыванию
2. Каждый узел дерева, кроме листьев, содержащий ключи `k1,...,kn`, имеет n+1 сына.
   i-й сын содержит ключи из отрезка `[ki−1;ki]`,`k0=−∞,kn+1=∞`
3. Все листья находятся на одном уровне
4. Каждый узел, кроме корня, содержит не менее t−1 ключей, и каждый
   внутренний узел имеет по меньшей мере t дочерних узлов
5. Каждый узел, кроме корня, содержит не более 2t−1 ключей и не более чем 2t
   сыновей во внутренних узлах.
6. Корень содержит от 1 до 2t−1 ключей, если дерево не пусто и от 2 до 2t детей
   при высоте большей 0.

## 13.7 B-дерево. Вставка

**Шаги для вставки в B-дерево:**

1. Найдите подходящий лист для вставки.
2. Вставьте ключ в лист.
3. Если лист переполнен, разделите его и поднимите средний ключ вверх.
4. Повторите разделение для родительского узла, если необходимо.
5. Если корень переполнен, создайте новый корень и разделите старый.

## 13.8 B-дерево. Удаление

B+ дерево - это B-дерево, в котором данные хранятся в листьях, а в промежуточных
вершинах хранятся только ключи.

B\* дерево - это улучшенная версия B-дерева. Узлы в B\* дереве содержат от
`(2/3) * (2t - 1)` до `2t - 1` ключей (где t - минимальная степень дерева). При
переполнении узла ключи перераспределяются между соседними узлами, что снижает
частоту разбиений и повышает эффективность операций вставки, удаления и поиска.

## 14.1 Графы

Граф - это математическая структура, состоящая из множества вершин (узлов) и
множества рёбер (связей) между ними, которая используется для моделирования
отношений между объектами.

### Примеры применения графов

* **Компьютерные сети**:
  * Моделирование сетевой топологии (узлы и соединения между ними).
  * Поиск путей для передачи данных (например, маршрутизация в IP-сетях).

* **Социальные сети**:
  * Моделирование пользователей и их взаимосвязей (друзья, подписчики).
  * Поиск путей для рекомендаций друзей (например, "друзья друзей").

* **Маршрутизация и навигация**:
  * Оптимизация маршрутов в GPS-навигаторах (например, алгоритмы Дейкстры и A*).
  * Моделирование дорожных сетей и поиск кратчайших путей.

* **Поиск информации**:
  * Структурирование данных в виде графов для веб-сканирования и индексирования
    (например, графы ссылок).
  * Алгоритмы PageRank для определения значимости веб-страниц.

* **Игровая разработка**:
  * Моделирование игровых уровней и взаимодействий (например, графы состояний
    для NPC).
  * Поиск путей для движения персонажей или объектов.

* **Оптимизация ресурсов**:
  * Задачи о максимальном потоке и минимальном разрезе в сети.
  * Задачи о назначении и распределении ресурсов.

* **Машинное обучение**:
  * Графовые нейронные сети для обработки данных, представленных в виде графов.
  * Алгоритмы кластеризации на графах для группировки данных.

* **Криптография**:
  * Моделирование сетевых протоколов и аутентификаций.
  * Алгоритмы для построения безопасных сетей.

### Понятия графа

**Обход графа** - посещение всех вершин графа.

**Ориентированный граф (Орграф)** - граф ребра, которого имеют направление.

**Взвешенный граф** - это граф, где каждому ребру поставлено значение (например
длина пути).

**Связные вершины** - это вершины между которыми есть путь.

**Связные компоненты** - множество связный вершин.

## 14.2 Представление графов

### Способы представления графа

* Список рёбер
* Матрица смежности
  * Быстрый способ, но требует O(n^2) памяти
* Списки смежных вершин

## 14.3 Обход графа в ширину

Есть два способа обхода графа:

* Поиск в глубину (DFS)
* Поиск в ширину (BFS)

Обход в ширину обходит вершины по уровням, от 0 до последнего.

С помощью поиска в ширину можно решить задачу нахождения кратчайшего пути к вершине
и задачи связанные с уровнями графов.

### Алгоритм поиска в ширину

* Берём вершину
* Обрабатываем её
* Посещаем соседей
* Кладём все непросмотренные вершины в **очередь**

## 14.4 Обход графа в глубину

С помощью обхода в глубину можно эффективно проверить, есть ли путь к вершине.

**Способы обхода графа:**

* DFS - Стек
* BFS - Очередь

### Алгоритм поиска в глубину

* Берём вершину
* Обрабатываем её
* Посещаем соседей
* Кладём все непросмотренные вершины в **стек**

Всё отличие от BFS в том что используется **стек**, а не очередь.

## 14.5 Прикладные задачи на алгоритмы обхода

### Алгоритм решения прикладных задач

1. Моделируем проблему в виде графа
2. Определяем, что нам надо найти
3. Выбираем подходящий алгоритм обхода (поиск в глубину или ширину)

Если данные можно представить с помощью объектов и связей между ними, есть вероятность
того, что задачу можно решить с помощью графов.

## 15.1 Алгоритмы на графах

Для орграфа понятие "компонента связности" неприменимо.

## 15.2 Топологическая сортировка

**Топологическая сортировка** - это упорядочение вершин ориентированного
ациклического графа (ОАГ) так, что для каждого ребра `u->v` вершина `u`

Топологическая сортировка может иметь несколько разных решений для одного графа.

При сортировке нужно проверять граф на цикл и закончить работу при его нахождении.

## 15.5 Алгоритм Дейкстры

**Алгоритм Дейкстры (Dijkstra's algorithm)** - алгоритм нахождения минимального пути
в взвешенном графе.

Суть алгоритма Дейкстры идти по вершинам, начиная с вершины с наименьшим расстоянием
к ней. Для каждой соседней вершины нужно переписать расстояние если оно меньше
текущего.

Алгоритм не работает с рёбрами с отрицательным весом. (Может зациклиться или найти
неправильный путь)

**Time Complexity:**

* `O(V^2)` with a simple array
* `O(V + E) log V)` with a binary heap
* `O(E + V log V)` with a Fibonacci heap

**Space Complexity:**

* `O(V)`

## 16.1 Сжатие текстовой информации. RLE

**Сжатие информации** - это операция, в результате которой исходное сообщение
уменьшается в объёме, но при этом сохраняется или повышается
качество информации.

Существует два типа сжатия информации:

* Сжатие с потерями
* Сжатие без потерь

RLE (Run-Length Encoding) - это способ сжатия данных, при котором подряд
идущие одинаковые элементы заменяются одним значением и числом их повторений.

## 16.2 Алгоритмы сжатия без потерь. Архивирование

**Архивирование** - это процедура, при которой сжатые данные
записываются в файлы с определённым расширением,
и при необходимости они легко могут быть восстановлены.

**Алгоритм Хаффмана** - это алгоритм сжатия данных без потерь.
Символ, который встречается в последовательности чаще всего,
получает новый очень маленький код, а символ, который встречается
реже всего, получает, наоборот, очень длинный код.

Для кодирования используется таблица Хаффмана, а для декодирования - дерево Хаффмана.

## 16.3 Алгоритмы сжатия с потерями. MP3 и JPEG

Особенности алгоритмов сжатия с потерями:

* Потеря и отсутствие механизмов восстановления исходных данных
* Возможность выбора степени сжатия

### Использование алгоритмов сжатия с потерями

* Аудио
* Видео
* Изображения
* При потоковой передаче данных
* Цифровая телефония

### Идеи в основе алгоритмов сжатия аудио

* Эффект маскировки
* Деление полосы звуковых частот на подполосы
* Использование психоакустической модели
* Совмещение стерео

**Аппроксимация** - это процесс нахождения приближенного значения функции или данных,
который позволяет упростить вычисления или анализ, сохраняя при этом
достаточную точность.

**Метод Фурье** - это математический подход, который разлагает периодические
функции на суммы синусоидальных компонентов, позволяя анализировать и
представлять сигналы в частотной области, что часто используется в алгоритмах
сжатия данных, например, в JPEG и MP3.

## 16.4 На каких данных какие алгоритмы работают лучше/хуже

**Алгоритмы RLE** лучше работают для данных, которые содержат длинные
последовательности символов или наборы символов.

**Алгоритм Хаффмана**, как и другие статические алгоритмы, хорошо работает на
данных с разными частотами встречаемости символов. При этом он хуже работает,
если все символы встречаются примерно одинаковое количество раз.

**Алгоритмы сжатия с потерями** применяют для файлов большого объёма, когда
требуется сжать файл с определённой степенью сжатия. Идея, на которой
основаны все алгоритмы сжатия с потерями: на первом этапе можно удалить
несущественную информацию, а на втором этапе к оставшимся данным -
применить наиболее подходящий алгоритм сжатия без потерь.

## 17.1 Представление данных

Отрицательные целые числа хранятся в памяти в **дополнительном коде**.

### Ключевые моменты

* **Старший бит**: 0 - положительное, 1 - отрицательное.
* **Получение**: Инвертировать биты числа и добавить 1.
* **Преимущества**: Упрощает арифметику (сложение и вычитание).

Пример: -5 в 8-битном формате: `11111011`.

## 17.2 Основные битовые операции

Основные битовые операции:

* `&` - and
* `|` - or
* `^` - xor
* `~` - not

## 17.4 Сложные битовые операции

**Обмен значений переменных:**

```php
<?php

$a ^= $b;
$b ^= $a;
$a ^= $b;
```

Особого применения у данного алгоритма нет.

**Проверка чисел на разницу в знаке:**

```php
<?php

function isSameSign(int $a, int $b): bool
{
    return ($a ^ $b) >= 0;
}
```

**Подсчёт количества бит в числе:**

```php
<?php

function countSetBits($a) {
    for ($ans = 0; $a !== 0; $ans++) {
        $a &= ($a - 1);
    }

    return $ans;
}
```

**Проверка на наличие нулевого байта (Алгоритм для строки в C):**

```php
<?php

$v = 0xFFEF01C2;
$hasZeroByte = ~(((($v & 0x7F7F7F7F) + 0x7F7F7F7F) | $v) | 0x7F7F7F7F);
```

## 17.5 Битовые маски

Для экономии места и для упрощения работы с несколькими флагами вместо `bool`,
который использует 1 байт, можно использовать бит-сет, где каждый флаг занимает
1 бит.

Таким образом представлены например разрешения для файла.

## 17.6 Как проходить собеседование

**Способы попасть на собеседование:**

* Откликнуться на вакансию
* Через референс знакомого

Для крупных компаний информацию по собеседованию можно найти в интернете.

### Типичные вопросы на собеседовании

* Behavioural questions
* Вопросы на теоретические знания
* Задачи на алгоритмы

## 17.7 Собеседование по алгоритмам

### Правила при решении задач по алгоритмам

1. Внимательно выслушать условие задачи; Желательно записать его
2. Задать дополнительные вопросы; В задачах могут быть специально не сказаны какие
   то условия
3. Пользоваться бумагой и ручкой
4. Если не получается придумать решение, можно перебрать алгоритмы на возможность
   применения
5. Если не удается придумать решение, придумайте простой, но медленный алгоритм
   (это лучше чем ничего не написать)
6. После того как решение придумано, стоит сказать его собеседующему, чтобы
   проверить правильное ли это решение (не стоит злоупотреблять этим)
7. Прежде чем писать решение, нужно продумать ключевые случаи (чтобы не переписывать
   большую часть кода, из-за того что он не поддерживает какой то случай)
8. После того как написано готовое решение, нужно проверить его на ошибки
9. Код с ошибками не будет считаться как полностью успешно решенная задача,
   поэтому нужно быть внимательным и писать код так чтобы в нем вообще не было ошибок
10. Неизвестно сколько задач запланировано на собеседование; Может быть одна
    может быть 10. Поэтому нужно стараться решить задачу как можно быстрее

### Что может пойти не так

* Могут быть опечатки, ошибки в синтаксисе и т.д.
* Не вспоминается название функции - в таком случае стоит сказать об этом собеседующему
* Можно запутаться и не закончить код - нужно больше практики, если всё очень
  плохо можно начать писать заново

Результат собеседования непредсказуем, поэтому не стоит заранее надеяться или
отчаиваться, а подождать результата собеседования.
Спорить с собеседующим стоит, но без агрессии.

## 18.1 Алгоритмы хеширования и вычисления контрольных сумм

Хеширование - это преобразование информации любой длины в строку фиксированного
размера, заданной алгоритмом функции хеширования.

Полученная строка называется **хеш-сумма** или **контрольная сумма**.

### Область применения хеширования данных

* Проверка целостности данных
* Криптография (в том числе аутентификация)
* Защита файлов
* Обнаружение вирусов
* Технология блокчейна и т.д.

### Алгоритмы хеширования

* `CRC-32` (8 цифр)
* `MD5` (32 цифры) - больше не рекомендуется для использования в криптографии
* `SHA`
  * `SHA1` (40 цифр) - больше не рекомендуется для использования в криптографии
  * `SHA256` (64 цифры) - Значительно более безопасен, чем `SHA1` и `MD5`

## 18.2 Криптографические алгоритмы

Алгоритмы шифрования

* Шифр Цезаря
* XOR-шифрование - операция xor с ключом
*

**Минусы использования шифроблокнотов:**

* Организация обмена ключами
* Генерация ключей

## 18.3 Симметричное и несимметричное шифрования

### Особенности симметричного шифрования

* 1 ключ
* Для внутреннего шифрования
* Плюс - Скорость
* Минус - Публичная передача ключей

### Особенности несимметричного шифрования

* Открытый и закрытый ключ
* Для передачи данных
* Плюс - Решает проблему передачи ключей
* Минус - Скорость

Один из наиболее безопасных симметричных алгоритмов шифрования - это AES
(Advanced Encryption Standard)

AES (Advanced Encryption Standard) - симметричный алгоритм блочного шифрования,
работающий с блоками по 128 бит и ключами длиной 128, 192 или 256 бит.
Алгоритм состоит из 10-14 раундов, в каждом из которых выполняются четыре
основные операции:

1. **Подстановка** (`SubBytes`) - замена байтов.
2. **Смешивание строк** (`ShiftRows`) - сдвиг строк.
3. **Смешивание столбцов** (`MixColumns`) - преобразование колонок.
u4. **Добавление раундового ключа** (`AddRoundKey`) - XOR с ключом.

AES обеспечивает высокую безопасность и скорость шифрования.

### Разновидности AES

* AES-128 - Размер блока 128 бит
* AES-256 - Размер блока 256 бит
* AES-512 - Размер блока 512 бит

Один из самых успешных алгоритмов асимметричного шифрования - RSA.

## 18.4 Вероятностные и итеративные алгоритмы

**Вероятностный алгоритм** - алгоритм, предусматривающий обращение на
определённых этапах своей работы к генератору случайных чисел для получения
экономии ресурсов за счёт замены абсолютной достоверности результата
достоверностью с некоторой вероятностью

**Методы Монте-Карло** - группа численных методов для изучения случайных
процессов. Суть метода: процесс описывается математической моделью с
использованием генератора случайных величин, модель многократно обсчитывается,
на основе полученных данных вычисляются вероятностные характеристики
рассматриваемого процесса.

**Алгоритм k-ближайших соседей** (`k-NN`) - алгоритм для автоматической
классификации объектов или регрессии. В случае использования метода для
классификации объект присваивается тому классу, который наиболее распространён
среди k-соседей данного элемента, классы которых уже известны. В случае
использования метода для регрессии объекту присваивается среднее значение по
k-ближайшим к нему объектам, значения которых уже известны

**Итеративный алгоритм** - это алгоритм, который решает задачу путем многократного
повторения одних и тех же шагов, постепенно приближаясь к решению.

## Dynamic Programming

Steps for solving dynamic programming problem:

1. Recursion
2. Store (Memoize)
3. Bottom-up

Characteristics of DP problems:

* Optimal substructures
* Overlapping subproblems

5 DP Indicators:

1. Sequential Decision-making
2. Greedy choice property
3. State transition
4. Path of array
5. Counting or Maximization / Minimization

Other similar techniques:

* Recursion
  * Direct Function Calls
  * Base and Recursive Cases
  * Simple but Inefficient
  * Limitations:
    * Redundant computations
    * Stack Overflow
* Divide-and-Conquer
  * Independent Subproblems
  * Divide, Conquer, Combine
  * Efficient for Independent Subproblems
  * Limitation - Lack of Overlap Handling

### Identifying Problems Suitable for Dynamic Programming

1. Check for Optimal Substructure
2. Check for Overlapping Subproblems
3. Design a Recursive Solution
4. Implement Memoization or Tabulation

### Key state concepts

* State Definition
* State Transition
* Base Cases

### Common Techniques for Space Optimization

Why space optimization is crucial:

* **Memory Constraints:** Large input sizes can lead to high memory usage,
  which might exceed available resources.
* **Improved Performance:** Reducing memory usage can lead to better cache
  utilization and faster access times.
* **Scalability:** Space-optimized algorithms can handle larger datasets more
  effectively, making them suitable for real-world applications.

Techniques to optimize space complexity in dynamic programming:

* Using Iterative Approaches Instead of Recursion
* Using Rolling Arrays
* State Compression
* Using In-Place Modifications

### Bellman-Ford Algorithm

The Bellman-Ford algorithm is a dynamic programming approach used to compute
the shortest paths from a single source vertex to all other vertices in a
weighted graph. It is particularly useful for graphs with negative weight edges,
as it can detect negative weight cycles.

**Key Characteristics:**

* **Time Complexity:** \(O(V \times E)\), where \(V\) is the number of
  vertices and \(E\) is the number of edges.
* **Handles Negative Weights:** Unlike Dijkstra's algorithm, Bellman-Ford can
  handle graphs with negative weight edges.
* **Detects Negative Cycles:** If there is a negative weight cycle, Bellman-Ford
  can identify it.

### Floyd-Warshall Algorithm

The Floyd-Warshall algorithm is an all-pairs shortest path algorithm. It finds
the shortest paths between every pair of vertices in a weighted graph.

**Key Characteristics:**

* **Time Complexity:** \(O(V^3)\), suitable for dense graphs.
* **Handles Negative Weights:** Can work with negative weights, but the graph
  must not have negative weight cycles.
* **Dynamic Programming Matrix:** Uses a 2D matrix to store shortest path
  estimates.

### Tips and Strategies for Identifying Dynamic Programming Problems

1. **Optimal Substructure:**
   * **Definition:** A problem has an optimal substructure if an optimal
     solution can be constructed efficiently from optimal solutions of its
     subproblems.
   * **Identification:** Look for problems where the solution can be built
     incrementally from solutions to smaller instances of the same problem.
     For example, in the shortest path problem, the shortest path to a node
     can be determined by the shortest path to its predecessor.

2. **Overlapping Subproblems:**
   * **Definition:** This occurs when the same subproblems are solved multiple
     times in a naive recursive approach.
   * **Identification:** If you find yourself solving the same problem
     repeatedly, this is a good indication that DP might be applicable. For
     example, calculating Fibonacci numbers recursively involves recalculating
     the same Fibonacci numbers multiple times.

3. **State Definition:**
   * **Strategy:** Define what the "state" of your problem is at any given
     point. The state should represent a subproblem whose solutions can be
     stored and reused.
   * **Example:** In the knapsack problem, a state might be defined by the
     remaining capacity of the knapsack and the items considered so far.

4. **State Transition:**
   * **Strategy:** Determine how to transition from one state to another. This
     usually involves deciding which decision leads from one subproblem to another.
   * **Example:** In the longest common subsequence problem, you transition
     between states by either including or excluding a character from one of
     the strings.

5. **Memoization or Tabulation:**
   * **Memoization:** This is a top-down approach where results of subproblems
     are stored in a table (usually a dictionary or array) as they are computed.
   * **Tabulation:** This is a bottom-up approach where a table is filled
     iteratively, avoiding recursion.
   * **Strategy:** Choose memoization if you prefer a recursive solution that
     is easier to implement and understand initially. Use tabulation if you
     want to avoid recursion and potentially reduce the overhead associated
     with recursive calls.

6. **Recursive Solution:**
   * **Strategy:** Before applying DP, try to write a recursive solution to
     the problem. This helps identify the subproblems and understand the
     problem structure. Once the recursive solution is understood, it can be
     transformed into a DP solution.

7. **Problem Types:**
   * **Common Problems:** Familiarize yourself with classic DP problems, such as:
     * **Fibonacci Sequence**
     * **Knapsack Problem**
     * **Coin Change Problem**
     * **Longest Common Subsequence**
     * **Matrix Chain Multiplication**
     * **Edit Distance**
   * **Patterns:** Recognize patterns in these problems, such as counting ways
     to reach a solution, finding optimal paths, or making decisions at each
     step to maximize or minimize a value.

8. **Trade-offs and Constraints:**
   * **Consider Constraints:** Check if the problem constraints allow for a DP
     solution. DP solutions are usually feasible when the problem size is
     small enough for the memoization or tabulation approach to be efficient.
   * **Space Complexity:** Decide if the space complexity is manageable. Some
     problems can be optimized to use less space by only storing necessary
     previous states.

9. **Iterative Refinement:**
   * **Strategy:** Begin with a brute force solution and iteratively refine it
     into a DP solution by identifying repeated calculations and optimizing them.

### Common Pitfalls

1. **Misidentifying a Problem as Suitable for DP**:
   * **Description**: Not every problem benefits from a dynamic programming
     approach. A common mistake is to try to force DP onto problems that are
     better solved with greedy algorithms, divide-and-conquer, or other strategies.
   * **Solution**: Look for the optimal substructure and overlapping
     subproblems. If a problem can be broken down into smaller subproblems
     that are reused in the process of solving the larger problem, it's likely
     a good candidate for DP.

2. **Incorrectly Defining the State**:
   * **Description**: A DP solution is built around the concept of states and
     their transitions. Defining an incorrect state can lead to incorrect or
     inefficient solutions.
   * **Solution**: Clearly define what each state represents and ensure it
     captures all the necessary information needed to make a transition to the
     next state.

3. **Inefficient State Transition**:
   * **Description**: Sometimes, the transition from one state to another is
     not optimized, leading to higher time complexity.
   * **Solution**: Analyze the problem constraints and look for ways to
     simplify transitions, such as by using memoization or bottom-up
     approaches to avoid redundant calculations.

4. **Improper Memoization**:
   * **Description**: Failing to memoize results properly can lead to
     excessive recomputation, negating the benefits of dynamic programming.
   * **Solution**: Use an appropriate data structure (e.g., arrays, hashes) to
     store computed results. Ensure that every recursive call checks if the
     result is already computed before proceeding.

5. **Boundary and Base Cases Mistakes**:
   * **Description**: Incorrectly handling base cases or boundary conditions
     can lead to errors or infinite loops.
   * **Solution**: Carefully consider the smallest instances of the problem
     and ensure they are correctly initialized. Test these cases thoroughly
     before scaling up.

6. **Incorrect Initialization**:
   * **Description**: Starting with incorrect initial values in your DP table
     can lead to wrong results.
   * **Solution**: Initialize your DP table with values that represent the
     solution to the simplest subproblems (often zero or negative infinity for
     maximization problems).

7. **Lack of Understanding of Recursive Relationships**:
   * **Description**: Failing to understand how the recursive relation works
     can lead to incorrect solutions.
   * **Solution**: Spend time deriving the recursive formula on paper,
     ensuring that it logically follows from the problem statement.

### Debugging Techniques

* Print Statements for State Values
* Small Test Cases
* Visualize the DP Table
* Compare with Brute Force
* Backtrack from Solution
